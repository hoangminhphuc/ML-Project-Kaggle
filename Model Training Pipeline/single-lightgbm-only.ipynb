{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":81933,"databundleVersionId":9643020,"sourceType":"competition"}],"dockerImageVersionId":30804,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport polars as pl\nimport pandas as pd\nfrom sklearn.base import clone\nfrom copy import deepcopy\nimport optuna\nfrom scipy.optimize import minimize\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport re\nfrom colorama import Fore, Style\nfrom sklearn.metrics import accuracy_score\nfrom tqdm import tqdm\nfrom IPython.display import clear_output\nfrom concurrent.futures import ThreadPoolExecutor\n\nimport warnings\nwarnings.filterwarnings('ignore')\npd.options.display.max_columns = None\n\nimport lightgbm as lgb\nfrom catboost import CatBoostRegressor, CatBoostClassifier\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import VotingRegressor\nfrom sklearn.model_selection import *\nfrom sklearn.metrics import *","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T13:20:41.438670Z","iopub.execute_input":"2024-12-19T13:20:41.438947Z","iopub.status.idle":"2024-12-19T13:20:41.450707Z","shell.execute_reply.started":"2024-12-19T13:20:41.438921Z","shell.execute_reply":"2024-12-19T13:20:41.449702Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Cleaning","metadata":{}},{"cell_type":"markdown","source":"\n<h2>Preparing the Data</h2>\r\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"font-size: 16px;\">\nBecause some standard machine learning models cannot natively handle time series data, we have to transform time series data into a vector by extracting summary statistic to simplifies data representation and less computational complexity\n</div>","metadata":{}},{"cell_type":"code","source":"def process_file(filename, dirname):\n    df = pd.read_parquet(os.path.join(dirname, filename, 'part-0.parquet')) \n    # Read all parquet files that ends with part-0.parquet in series-train.parquet directory\n    df.drop('step', axis=1, inplace=True)\n    return df.describe().values.reshape(-1), filename.split('=')[1]\n\ndef load_time_series(dirname) -> pd.DataFrame:\n    ids = os.listdir(dirname)\n    \n    with ThreadPoolExecutor() as executor:\n        results = list(tqdm(executor.map(lambda fname: process_file(fname, dirname), ids), total=len(ids)))\n    # results is a tuple of statistic value with its id, like ([10.5, 20.3, 15.7], '001'),([12.1, 18.6, 14],'002')  \n    stats, indexes = zip(*results)\n    # now stats hold [10.5, 20.3, 15.7],... and indexes hold 001, 002,..\n    df = pd.DataFrame(stats, columns=[f\"Stat_{i}\" for i in range(len(stats[0]))])\n    df['id'] = indexes\n    \n    return df\n\ntrain = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\ntest = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')\nsample = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/sample_submission.csv')\n\ntrain_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\")\ntest_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T13:20:41.452163Z","iopub.execute_input":"2024-12-19T13:20:41.452559Z","iopub.status.idle":"2024-12-19T13:22:56.688935Z","shell.execute_reply.started":"2024-12-19T13:20:41.452514Z","shell.execute_reply":"2024-12-19T13:22:56.687105Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"time_series_cols = train_ts.columns.tolist()\ntime_series_cols.remove(\"id\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T13:22:56.691057Z","iopub.execute_input":"2024-12-19T13:22:56.691576Z","iopub.status.idle":"2024-12-19T13:22:56.698215Z","shell.execute_reply.started":"2024-12-19T13:22:56.691520Z","shell.execute_reply":"2024-12-19T13:22:56.696921Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train = pd.merge(train, train_ts, how=\"left\", on='id')\ntest = pd.merge(test, test_ts, how=\"left\", on='id')\n#We merge because train and train_ts, both of them have rows that correspond to a single child (with id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T13:22:56.699548Z","iopub.execute_input":"2024-12-19T13:22:56.699968Z","iopub.status.idle":"2024-12-19T13:22:56.723433Z","shell.execute_reply.started":"2024-12-19T13:22:56.699936Z","shell.execute_reply":"2024-12-19T13:22:56.722249Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# After merging we drop it cause we no longer need it\ntrain = train.drop('id', axis=1)\ntest = test.drop('id', axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T13:22:56.724848Z","iopub.execute_input":"2024-12-19T13:22:56.725296Z","iopub.status.idle":"2024-12-19T13:22:56.735793Z","shell.execute_reply.started":"2024-12-19T13:22:56.725248Z","shell.execute_reply":"2024-12-19T13:22:56.734787Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"font-size: 16px;\">\nDropping all the PCIAT-related columns because they are not in the test set\n</div>","metadata":{}},{"cell_type":"code","source":"featuresCols = [\n                'Basic_Demos-Enroll_Season', 'Basic_Demos-Age', 'Basic_Demos-Sex',\n                'CGAS-Season', 'CGAS-CGAS_Score', 'Physical-Season', 'Physical-BMI',\n                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n                'Fitness_Endurance-Season',\n                'Fitness_Endurance-Max_Stage','Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n                'FGC-Season', 'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone', 'BIA-Season',\n                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n                'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n                'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n                'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n                'BIA-BIA_TBW', \n                'PAQ_A-Season', 'PAQ_A-PAQ_A_Total', \n                'PAQ_C-Season',\n                'PAQ_C-PAQ_C_Total', 'SDS-Season', 'SDS-SDS_Total_Raw',\n                'SDS-SDS_Total_T', 'PreInt_EduHx-Season',\n                'PreInt_EduHx-computerinternet_hoursday', 'sii']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T13:22:56.737168Z","iopub.execute_input":"2024-12-19T13:22:56.737514Z","iopub.status.idle":"2024-12-19T13:22:56.745424Z","shell.execute_reply.started":"2024-12-19T13:22:56.737483Z","shell.execute_reply":"2024-12-19T13:22:56.744364Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"featuresCols += time_series_cols","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T13:22:56.746750Z","iopub.execute_input":"2024-12-19T13:22:56.747104Z","iopub.status.idle":"2024-12-19T13:22:56.761532Z","shell.execute_reply.started":"2024-12-19T13:22:56.747073Z","shell.execute_reply":"2024-12-19T13:22:56.760569Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train = train[featuresCols]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T13:22:56.762649Z","iopub.execute_input":"2024-12-19T13:22:56.762968Z","iopub.status.idle":"2024-12-19T13:22:56.775894Z","shell.execute_reply.started":"2024-12-19T13:22:56.762939Z","shell.execute_reply":"2024-12-19T13:22:56.775037Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h2>Handling missing values</h2>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"font-size: 16px;\">\nWe are dropping features that have more than 80% missing values\n</div>","metadata":{}},{"cell_type":"code","source":"threshold = 0.8\ntarget = train['sii']\ntrain = train[[col for col in train.columns if col != 'sii']].dropna(axis=1, thresh=(1 - threshold) * len(train))\ntest = test[train.columns] \ntrain['sii'] = target","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T13:22:56.777061Z","iopub.execute_input":"2024-12-19T13:22:56.777354Z","iopub.status.idle":"2024-12-19T13:22:56.800256Z","shell.execute_reply.started":"2024-12-19T13:22:56.777326Z","shell.execute_reply":"2024-12-19T13:22:56.799050Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h2>Removing Outliers</h2>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"font-size: 16px;\">\nReplacing outliers in CGAS-CGAS_Score with NaN value\n</div>","metadata":{}},{"cell_type":"code","source":"train[train['CGAS-CGAS_Score'] > 100]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T13:22:56.801788Z","iopub.execute_input":"2024-12-19T13:22:56.802245Z","iopub.status.idle":"2024-12-19T13:22:56.873070Z","shell.execute_reply.started":"2024-12-19T13:22:56.802199Z","shell.execute_reply":"2024-12-19T13:22:56.872020Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.loc[train['CGAS-CGAS_Score'] == 999, 'CGAS-CGAS_Score'] = np.nan","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T13:22:56.874416Z","iopub.execute_input":"2024-12-19T13:22:56.874712Z","iopub.status.idle":"2024-12-19T13:22:56.880952Z","shell.execute_reply.started":"2024-12-19T13:22:56.874685Z","shell.execute_reply":"2024-12-19T13:22:56.879904Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"font-size: 16px;\">\nReplacing zero stats in these physical features as NaN value\n</div>","metadata":{}},{"cell_type":"code","source":"physical_cols = [\n    'Physical-BMI', 'Physical-Height',\n    'Physical-Weight', 'Physical-Waist_Circumference'\n]\nprint((train[physical_cols] == 0).sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T13:22:56.882243Z","iopub.execute_input":"2024-12-19T13:22:56.882560Z","iopub.status.idle":"2024-12-19T13:22:56.901855Z","shell.execute_reply.started":"2024-12-19T13:22:56.882531Z","shell.execute_reply":"2024-12-19T13:22:56.900689Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train[physical_cols] = train[physical_cols].replace(0, np.nan)\nprint((train[physical_cols] == 0).sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T13:22:56.903286Z","iopub.execute_input":"2024-12-19T13:22:56.903728Z","iopub.status.idle":"2024-12-19T13:22:56.921150Z","shell.execute_reply.started":"2024-12-19T13:22:56.903681Z","shell.execute_reply":"2024-12-19T13:22:56.920175Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"bp_cols = [\n      'Physical-Diastolic_BP', 'Physical-Systolic_BP'\n]\ntrain[bp_cols] = train[bp_cols].replace(0, np.nan)\ntrain.loc[train['Physical-Systolic_BP'] <= train['Physical-Diastolic_BP'], bp_cols] = np.nan","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T13:22:56.922446Z","iopub.execute_input":"2024-12-19T13:22:56.922839Z","iopub.status.idle":"2024-12-19T13:22:56.937133Z","shell.execute_reply.started":"2024-12-19T13:22:56.922807Z","shell.execute_reply":"2024-12-19T13:22:56.936068Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"font-size: 16px;\">\nReplacing extreme outliers in BIA-related columns with NaN values.\n</div>","metadata":{}},{"cell_type":"code","source":"bia_columns = [col for col in train.columns if 'BIA' in col]\n\ncat_col = ['BIA-Season', 'BIA-BIA_Activity_Level_num', 'BIA-BIA_Frame_num']\nbia_columns = [col for col in bia_columns if col not in cat_col]\nbia_columns\n\nexclude_rows = []  # To store all indices to exclude\n\nfor col in bia_columns:\n    Q1 = train[col].quantile(0.001)  # Lower quantile\n    Q3 = train[col].quantile(0.999)  # Upper quantile\n\n    # Replace with nan \n    train.loc[train[col] > Q3, col] = np.nan  # Replace above Q3 with NaN\n    train.loc[train[col] < Q1, col] = np.nan  # Replace below Q1 with NaN\n\n#  Dropping rows \n#     exclude_q3 = train[train[col] > Q3].index.tolist()\n#     exclude_q1 = train[train[col] < Q1].index.tolist()\n    \n#     exclude_rows += exclude_q3 + exclude_q1\n\n# exclude_rows = list(set(exclude_rows))\n\n# train = train.drop(index=exclude_rows)\n\ntrain","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T13:22:56.938532Z","iopub.execute_input":"2024-12-19T13:22:56.938844Z","iopub.status.idle":"2024-12-19T13:22:57.120023Z","shell.execute_reply.started":"2024-12-19T13:22:56.938813Z","shell.execute_reply":"2024-12-19T13:22:57.119045Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"markdown","source":"<h2>Feature Engineering</h2>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"font-size: 16px;\">\nDropping features have importance = 0 when training Light model based on different seeds\n</div>","metadata":{}},{"cell_type":"code","source":"# Dropping features have importance = 0 when training Light model based on different seeds\n\ncolumns_drop = ['Stat_39', 'Stat_45', 'Stat_41', 'Stat_89', 'Stat_6', 'Stat_42', \n                 'Stat_7', 'Stat_10', 'Stat_9', 'Stat_11'\n                , 'Stat_93', 'Stat_8'\n               ]\ntrain = train.drop(columns = columns_drop)\ntest = test.drop(columns = columns_drop)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T13:22:57.121246Z","iopub.execute_input":"2024-12-19T13:22:57.121530Z","iopub.status.idle":"2024-12-19T13:22:57.130930Z","shell.execute_reply.started":"2024-12-19T13:22:57.121503Z","shell.execute_reply":"2024-12-19T13:22:57.129903Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Re-creating interaction features\n\n# # Interaction between Physical-Height and Physical-Weight\n# train[\"Height_Weight\"] = train[\"Physical-Height\"] * train[\"Physical-Weight\"]\n\n# # Interaction between Basic_Demos-Age and Internet_Use_Hours\n# train[\"Age_Internet_Hours\"] = train[\"Basic_Demos-Age\"] * train[\"PreInt_EduHx-computerinternet_hoursday\"]\n\n# # Interaction between Physical-Height and Basic_Demos-Age\n# train[\"Height_Age\"] = train[\"Physical-Height\"] * train[\"Basic_Demos-Age\"]\n\n# # Interaction between Physical-Weight and Fitness-Endurance-Time_Sec\n# train[\"Weight_Endurance\"] = train[\"Physical-Weight\"] * train[\"Physical-Height\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T13:22:57.132365Z","iopub.execute_input":"2024-12-19T13:22:57.132672Z","iopub.status.idle":"2024-12-19T13:22:57.142002Z","shell.execute_reply.started":"2024-12-19T13:22:57.132643Z","shell.execute_reply":"2024-12-19T13:22:57.140741Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def feature_transformation(data):\n    # data['Height_squared'] = data['Physical-Height'] ** 2\n    # data['calculated_BMI'] = data['Physical-Weight'] / ((data['Physical-Height'] ** 2) + 1e-5)\n    # data['BP_Ratio'] = data['Physical-Systolic_BP'] / (data['Physical-Diastolic_BP'] + 1e-5)\n    # data['Fat_to_Lean_Ratio'] = data['BIA-BIA_FMI'] / (data['BIA-BIA_FFMI'] + 1e-5)\n    # data['Fat_to_LST_Ratio'] = data['BIA-BIA_Fat'] / (data['BIA-BIA_LST'] + 1e-5)\n    # data['Water_Balance'] = data['BIA-BIA_ICW'] / (data['BIA-BIA_ECW'] + 1e-5)\n    # data['ICW_TBW_Ratio'] = data['BIA-BIA_ICW'] / data['BIA-BIA_TBW']\n    # data['ECW_TBW_Ratio'] = data['BIA-BIA_ECW'] / data['BIA-BIA_TBW']\n    # data['Metabolic_Efficiency'] = data['BIA-BIA_DEE'] / (data['BIA-BIA_BMR'] + 1e-5)\n    # data['SMM_to_LDM_Ratio'] = data['BIA-BIA_SMM'] / (data['BIA-BIA_LDM'] + 1e-5)\n    # data['Total_Body_Mass'] = data['BIA-BIA_FFM'] + data['BIA-BIA_Fat']\n    # data['Activity_Sleep_Ratio'] = data['PAQ_C-PAQ_C_Total'] / (data['SDS-SDS_Total_Raw'] + 1e-5)\n    # data['Strength_Efficiency'] = data['FGC-FGC_CU'] / (data['PAQ_C-PAQ_C_Total'] + 1e-5)\n    # data['FGC_PAQ_C_Interaction'] = data['FGC-FGC_CU'] * data['PAQ_C-PAQ_C_Total']\n\n    # col = [\n    #     'Physical-Systolic_BP', 'Physical-Diastolic_BP', 'BIA-BIA_FMI', 'BIA-BIA_FFMI', 'BIA-BIA_Fat',\n    #     'BIA-BIA_LST', 'BIA-BIA_ICW', 'BIA-BIA_ECW'\n    # ]\n    # data = data.drop(columns = col, axis = 1)\n\n    # data['Age_HeartRate_Ratio'] = data['Physical-HeartRate'] / (data['Basic_Demos-Age'] + 1e-5)\n    # data['Activity_SDS_Impact'] = data['PAQ_C-PAQ_C_Total'] / (data['SDS-SDS_Total_Raw'] + 1e-5)\n    # data['Age_SDS_Interaction'] = data['Basic_Demos-Age'] * data['SDS-SDS_Total_Raw']\n    # data['Age_Internet_Use'] = data['PreInt_EduHx-computerinternet_hoursday'] * data['Basic_Demos-Age']\n    # data['Weight_Increase'] = data['Basic_Demos-Age'] * data['Physical-Weight']\n    # data['Height_Increase'] = data['Basic_Demos-Age'] * data['Physical-Height']\n\n    # data['BMI_difference'] = data['Physical-BMI'] - data['calculated_BMI']\n    return data\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T13:22:57.152258Z","iopub.execute_input":"2024-12-19T13:22:57.152965Z","iopub.status.idle":"2024-12-19T13:22:57.162484Z","shell.execute_reply.started":"2024-12-19T13:22:57.152929Z","shell.execute_reply":"2024-12-19T13:22:57.161537Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#train = feature_transformation(train)\n#test = feature_transformation(test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T13:22:57.163657Z","iopub.execute_input":"2024-12-19T13:22:57.163998Z","iopub.status.idle":"2024-12-19T13:22:57.179497Z","shell.execute_reply.started":"2024-12-19T13:22:57.163949Z","shell.execute_reply":"2024-12-19T13:22:57.178321Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"font-size: 16px;\">\nDropping out rows that contain NaN value for target column\n</div>","metadata":{}},{"cell_type":"code","source":"# train = train[featuresCols]\ntrain = train.dropna(subset='sii')\ntrain","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T13:22:57.180866Z","iopub.execute_input":"2024-12-19T13:22:57.181255Z","iopub.status.idle":"2024-12-19T13:22:57.308472Z","shell.execute_reply.started":"2024-12-19T13:22:57.181224Z","shell.execute_reply":"2024-12-19T13:22:57.307261Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"font-size: 18px;\">\nHere we perform encoding the categorical features\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"font-size: 16px;\">\nFill the NaN values of categorical features as <strong>Missing</strong>\n</div>","metadata":{}},{"cell_type":"code","source":"cat_c = ['Basic_Demos-Enroll_Season', 'CGAS-Season', 'Physical-Season', 'Fitness_Endurance-Season', \n          'FGC-Season', 'BIA-Season', \n         'PAQ_A-Season', \n         'PAQ_C-Season', 'SDS-Season', 'PreInt_EduHx-Season']\nfinal_cat_columns = [col for col in cat_c if col in train.columns]\n\ndef update(df):\n    for c in final_cat_columns: \n        df[c] = df[c].fillna('Missing')\n        df[c] = df[c].astype('category')\n    return df\n\ntrain = update(train)\ntest = update(test)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T13:22:57.309751Z","iopub.execute_input":"2024-12-19T13:22:57.310099Z","iopub.status.idle":"2024-12-19T13:22:57.339574Z","shell.execute_reply.started":"2024-12-19T13:22:57.310067Z","shell.execute_reply":"2024-12-19T13:22:57.338495Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"font-size: 16px;\">\nMapping to numerical values for the model to train\n</div>","metadata":{}},{"cell_type":"code","source":"def create_mapping(column, dataset):\n    unique_values = dataset[column].unique()\n# enumerate adds index to an iterable. unique_values = ['A','B','C'] => enumerate returns [(0, 'A'), (1, 'B'), (2, 'C')]\n    return {value: idx for idx, value in enumerate(unique_values)}\n\n# Similar to Label Encoder\nfor col in final_cat_columns:\n    all_values = pd.concat([train[col], test[col]]).unique()\n    mapping = {value: idx for idx, value in enumerate(all_values)}\n\n    train[col] = train[col].replace(mapping).astype(int)\n    test[col] = test[col].replace(mapping).astype(int)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T13:22:57.340789Z","iopub.execute_input":"2024-12-19T13:22:57.341135Z","iopub.status.idle":"2024-12-19T13:22:57.387252Z","shell.execute_reply.started":"2024-12-19T13:22:57.341103Z","shell.execute_reply":"2024-12-19T13:22:57.386172Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# test = test[train.columns.drop('sii')]  # Align test columns to training features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T13:22:57.388579Z","iopub.execute_input":"2024-12-19T13:22:57.388910Z","iopub.status.idle":"2024-12-19T13:22:57.393601Z","shell.execute_reply.started":"2024-12-19T13:22:57.388880Z","shell.execute_reply":"2024-12-19T13:22:57.392509Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Training and Evaluation","metadata":{}},{"cell_type":"code","source":"SEED = 42\nn_splits = 5","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T13:22:57.394746Z","iopub.execute_input":"2024-12-19T13:22:57.395109Z","iopub.status.idle":"2024-12-19T13:22:57.407889Z","shell.execute_reply.started":"2024-12-19T13:22:57.395078Z","shell.execute_reply":"2024-12-19T13:22:57.406517Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Low importance features across different seeds\nlow_imp_features = []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T13:22:57.409205Z","iopub.execute_input":"2024-12-19T13:22:57.409563Z","iopub.status.idle":"2024-12-19T13:22:57.418363Z","shell.execute_reply.started":"2024-12-19T13:22:57.409533Z","shell.execute_reply":"2024-12-19T13:22:57.417297Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# High importance features across different seeds\nhigh_imp_features = []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T13:22:57.419731Z","iopub.execute_input":"2024-12-19T13:22:57.420173Z","iopub.status.idle":"2024-12-19T13:22:57.430003Z","shell.execute_reply.started":"2024-12-19T13:22:57.420128Z","shell.execute_reply":"2024-12-19T13:22:57.428928Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h2>Supporting Functions</h2>\n    <ul style=\"font-size: 16px;\">\n        <li><strong>quadratic_weighted_kappa</strong>: Calculates the QWK score.</li>\n        <li><strong>threshold_Rounder</strong>: Rounds the <code>sii</code> from <code>PCIAT_Total</code> to correct labels.</li>\n        <li><strong>evaluate_predictions</strong>: Combines the two functions above to round continuous predictions and return the negative QWK score for optimization using the minimize library.</li>\n    </ul>","metadata":{}},{"cell_type":"markdown","source":" <h2>TrainML Function</h2>\n    <p style=\"font-size: 16px;\">The <strong>TrainML</strong> function performs the following tasks:</p>\n        <ul style=\"font-size: 16px;\">\n        <li>Implements stratified k-fold cross-validation to train and validate the given model.</li>\n        <li>Trains a cloned instance of the input model on training data for each fold.</li>\n        <li>Calculates and prints QWK scores for both training and validation sets.</li>\n        <li>Generates predictions for the test dataset during each fold and averages them.</li>\n        <li>Optimizes the thresholds for rounding predictions to maximize QWK using a threshold tuning function and the Nelder-Mead optimization method.</li>\n    </ul>","metadata":{}},{"cell_type":"code","source":"%%time\n\n\n#We need Confusion matrix, weight matrix and expected matrix\ndef quadratic_weighted_kappa(y_true, y_pred):\n    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n\n\ndef threshold_Rounder(oof_non_rounded, thresholds):\n    return np.where(oof_non_rounded < thresholds[0], 0,\n                    np.where(oof_non_rounded < thresholds[1], 1,\n                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n\n\n# Optimization libraries like scipy.optimize.minimize work by minimizing a function. \n# Since QWK is a metric where higher is better, we negate it to allow the optimizer to \"maximize\" QWK indirectly.\ndef evaluate_predictions(thresholds, y_true, oof_non_rounded):\n    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n    return -quadratic_weighted_kappa(y_true, rounded_p)\n\n# TrainML, is a machine learning pipeline that performs model training, cross-validation, evaluation, \n# and test set prediction for a classification or regression task. \n\ndef TrainML(model_class, test_data\n            # , k_values=[5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n           ):\n    \n    X = train.drop(['sii'], axis=1)\n    y = train['sii']\n\n    # best_kappa = -np.inf\n    # best_k = None\n    # best_model = None\n    # best_submission = None\n\n    # for n_splits in k_values:\n    #     print(f\"\\nEvaluating for k = {n_splits} folds...\")\n\n    # Cross-validation, ensuring that the folds have the same distribution of target classes. \n    SKF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n    \n    # Store QWK scores for training and validation sets.\n    train_S = []\n    test_S = []\n\n    # predictions made by the model for the validation set during each fold of cross-validation.\n    oof_non_rounded = np.zeros(len(y), dtype=float) \n    # discrete predictions derived from oof_non_rounded using rounding logic\n    oof_rounded = np.zeros(len(y), dtype=int) \n    # Stores the model's predictions for the test dataset during each fold of cross-validation\n    test_preds = np.zeros((len(test_data), n_splits))\n\n    for fold, (train_idx, test_idx) in enumerate(tqdm(SKF.split(X, y), desc=\"Training Folds\", total=n_splits)):\n        # train and test index are the index of this fold in the original training set\n        # Data used for training in this fold\n        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n        # Data used for validation in this fold\n        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n        \n        \n        # Creates a new instance of an estimator with the same parameters as the original one \n        # but without any of the fitted data or state. Ensures each fold starts with a clean estimator.\n        model = clone(model_class)\n        model.fit(X_train, y_train)\n\n        \n        # Predict after fit to ensure model is trained correctly\n        y_train_pred = model.predict(X_train)\n\n        y_train_pred_rounded = y_train_pred.round(0).astype(int)\n        \n        y_val_pred = model.predict(X_val)\n\n        # stores continuous data\n        oof_non_rounded[test_idx] = y_val_pred\n        # round the predicted value\n        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n        oof_rounded[test_idx] = y_val_pred_rounded\n\n        # Calcute QWK for train and validation set\n        train_kappa = quadratic_weighted_kappa(y_train, y_train_pred_rounded)\n        val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)\n\n        # Stores QWK score each fold\n        train_S.append(train_kappa)\n        test_S.append(val_kappa)\n\n        \n        test_preds[:, fold] = model.predict(test_data)\n        print(test_preds[:, fold])\n        \n        print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n        clear_output(wait=True)\n\n\n    print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n    print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n\n    #Minimize the kappa score, which find the best Threshold Rounder  \n    KappaOPtimizer = minimize(evaluate_predictions,\n                              x0=[0.5, 1.5, 2.5], args=(y, oof_non_rounded), \n                              method='Nelder-Mead') # Nelder-Mead | # Powell\n\n    # If not converge, raise error\n    assert KappaOPtimizer.success, \"Optimization did not converge.\"\n\n# rounded the oof_non_rounded which has a bunch of predicted continuous variables for the validation set at each fold\n    oof_tuned = threshold_Rounder(oof_non_rounded, KappaOPtimizer.x)\n    tKappa = quadratic_weighted_kappa(y, oof_tuned)\n\n    \n\n    print(f\"----> || Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n\n# Rounded the test predict variables with optimized ThresHold_Rounder\n    tpm = test_preds.mean(axis=1)\n    tpTuned = threshold_Rounder(tpm, KappaOPtimizer.x)\n    \n    submission = pd.DataFrame({\n        'id': sample['id'],\n        'sii': tpTuned\n    })\n\n        # if tKappa > best_kappa:\n        #     best_kappa = tKappa\n        #     best_k = n_splits\n        #     best_model = model\n        #     best_submission = submission\n\n    # print(f\"Best k value: {best_k} with QWK Score: {best_kappa:.4f}\")\n\n    return submission,model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T13:22:57.431933Z","iopub.execute_input":"2024-12-19T13:22:57.432438Z","iopub.status.idle":"2024-12-19T13:22:57.454289Z","shell.execute_reply.started":"2024-12-19T13:22:57.432392Z","shell.execute_reply":"2024-12-19T13:22:57.453098Z"},"_kg_hide-input":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n\n# LightParams = {'num_leaves': 443, 'max_depth': 11, 'learning_rate': 0.03975148176099325, \n#                'feature_fraction': 0.7369224778898958, 'bagging_fraction': 0.7673760625886821, \n#            'bagging_freq': 2, 'lambda_l1': 4.329242178275905, 'lambda_l2': 8.559080320318334e-06, \n#                'min_data_in_leaf': 11\n# }\nLightParams = {\n    'num_leaves': 484, 'max_depth': 11, 'learning_rate': 0.04533585929025977, \n    'feature_fraction': 0.8110477902071817, 'bagging_fraction': 0.7265461551046623, 'bagging_freq': 2, \n    'lambda_l1': 5.338437863405547, 'lambda_l2': 4.499492326361118e-06, 'min_data_in_leaf': 13\n}\n# train 0.8276 cv 0.4067 opt qwk 0.453\n\n# LightParams = {\n#     'num_leaves': 405, 'max_depth': 14, 'learning_rate': 0.04144286287843841,\n#     'feature_fraction': 0.7449300074156723, 'bagging_fraction': 0.728943178243488, 'bagging_freq': 2, \n#     'lambda_l1': 5.47005971080022, 'lambda_l2': 8.207849169164218e-06, 'min_data_in_leaf': 11\n# } train 0.8201 cv 0.3971 opt qwk 0.452\n\nLight = lgb.LGBMRegressor(**LightParams,random_state=SEED, verbose=-1,n_estimators=200)\n\nSubmission,model = TrainML(Light,test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T13:22:57.455590Z","iopub.execute_input":"2024-12-19T13:22:57.455905Z","iopub.status.idle":"2024-12-19T13:23:11.154266Z","shell.execute_reply.started":"2024-12-19T13:22:57.455877Z","shell.execute_reply":"2024-12-19T13:23:11.153196Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Feature Importances","metadata":{}},{"cell_type":"code","source":"feature_importances = pd.DataFrame({\n    'Feature': model.booster_.feature_name(),\n    'Importance': model.booster_.feature_importance(importance_type='gain')\n}).sort_values(by='Importance', ascending=False)\n\nfeature_importances","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T13:23:11.156044Z","iopub.execute_input":"2024-12-19T13:23:11.156484Z","iopub.status.idle":"2024-12-19T13:23:11.172125Z","shell.execute_reply.started":"2024-12-19T13:23:11.156437Z","shell.execute_reply":"2024-12-19T13:23:11.171037Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"font-size: 16px;\">\n    Here i examine the high and low importance features accross different seeds, and perform feature engineering \n</div>","metadata":{}},{"cell_type":"code","source":"# high_importance_features = feature_importances['Feature'].head(10)\n# high_importance_features = high_importance_features.tolist()\n\n# high_imp_features.append(high_importance_features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T13:23:11.173299Z","iopub.execute_input":"2024-12-19T13:23:11.173667Z","iopub.status.idle":"2024-12-19T13:23:11.182630Z","shell.execute_reply.started":"2024-12-19T13:23:11.173636Z","shell.execute_reply":"2024-12-19T13:23:11.181309Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from itertools import combinations\n\n# # Function to compute distinct common strings\n# def find_distinct_common_strings(array):\n#     results = {}\n\n#     n = len(array)  # Total number of lists in the array\n\n#     # Convert all lists to sets for easier intersection\n#     sets = [set(lst) for lst in array]\n\n#     # Keep track of already identified strings\n#     identified_strings = set()\n\n#     # For each exclusion level (0 to n-1)\n#     for exclude_count in range(n):\n#         current_level_strings = set()\n        \n#         # Generate combinations of lists with `exclude_count` excluded\n#         for excluded_indices in combinations(range(n), exclude_count):\n#             # Include only the indices not in excluded_indices\n#             included_sets = [sets[i] for i in range(n) if i not in excluded_indices]\n#             # Find intersection of included sets\n#             if included_sets:\n#                 intersection = set.intersection(*included_sets)\n#                 # Exclude strings already identified in previous levels\n#                 intersection -= identified_strings\n#                 current_level_strings.update(intersection)\n        \n#         # Store results for this exclusion level\n#         level_name = f\"length-{n - exclude_count}\"\n#         results[level_name] = current_level_strings\n\n#         # Update identified strings to include the current level's strings\n#         identified_strings.update(current_level_strings)\n\n#     return results\n\n# distinct_common_strings = find_distinct_common_strings(high_imp_features)\n\n# for key, value in distinct_common_strings.items():\n#     print(f\"{key}: {value}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T13:23:11.184010Z","iopub.execute_input":"2024-12-19T13:23:11.184339Z","iopub.status.idle":"2024-12-19T13:23:11.195877Z","shell.execute_reply.started":"2024-12-19T13:23:11.184309Z","shell.execute_reply":"2024-12-19T13:23:11.194915Z"},"_kg_hide-input":true,"_kg_hide-output":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Submission.to_csv('submission.csv', index=False)\nprint(Submission['sii'].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T13:23:11.197287Z","iopub.execute_input":"2024-12-19T13:23:11.197725Z","iopub.status.idle":"2024-12-19T13:23:11.219781Z","shell.execute_reply.started":"2024-12-19T13:23:11.197683Z","shell.execute_reply":"2024-12-19T13:23:11.218707Z"}},"outputs":[],"execution_count":null}]}